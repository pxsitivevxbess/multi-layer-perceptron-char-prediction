{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cf957494-499b-42c5-9b40-b758d85baa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "aa8d59ff-f57c-4479-be0d-d3ff1b2af68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBJECTIVE: WE WANT TO PREDICT NEXT CHARATCER BY TAKING CONTEXT FROM MORE PREVIOUS CHARACTERS(like 3) INSTEAD OF JUST 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7f4acce9-9688-44a4-9004-bd5f9d24b8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1 read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "82fe25ad-337b-4cd9-8735-0d29a03f3d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "charIntIndexMapping = {s:i+1 for i,s in enumerate(chars)}\n",
    "charIntIndexMapping['.'] = 0\n",
    "indexToCharMapping ={i:s for s,i in charIntIndexMapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3d8758cc-88fb-4fbf-96d0-2b111083321c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 3])\n",
      "torch.Size([228146])\n"
     ]
    }
   ],
   "source": [
    "#2 BUILDING TRAINING DATASET\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one? basically size of sliding window\n",
    "inputContext, outputCharcterForThatContext = [], []\n",
    "\n",
    "for w in words:\n",
    "  \n",
    "\n",
    "  contextSlidingWindow = [0] * block_size #Initialise with arr= [0,0,0] where 0 represents '.' char so essentially \"...\"\n",
    "    \n",
    "  for ch in w + '.': #Appending '.' to mark end of word\n",
    "      \n",
    "    #NOTE ON APPENDING '.' in Words:\n",
    "    #1. In bigram model we were appending '.' to start and end of each word \n",
    "    #2. Here we dont need to append '.' in starting because we are starting context as \"...\" \n",
    "\n",
    "      \n",
    "    index = charIntIndexMapping[ch]\n",
    "    inputContext.append(contextSlidingWindow)\n",
    "    outputCharcterForThatContext.append(index)\n",
    "    #print(''.join(indexToCharMapping[i] for i in context), '--->', indexToCharMapping[ix])\n",
    "    \n",
    "    contextSlidingWindow = contextSlidingWindow[1:] + [index] #Updating context, so context is like sliding window of size=block_size, when you move to \n",
    "    #next iteration you append current char from back in sliding window and first char is removed\n",
    "    #Syntax context[1:] means taking array from index-1 to last index and appending index represented by current char\n",
    "\n",
    "#1. inputContext: Represents a matrix where row represents integer form of each possible sliding window of size -3 in all name in names.txt\n",
    "# and we have 3 columns(bec context_length=3) which represents the content of contextWindow represented by row\n",
    "\n",
    "\n",
    "#Forexample for emma actually becomes emma.\n",
    "#Row  Columns(int mapping of char)  contextWindow  \n",
    "#0->  0,0,0                         ...\n",
    "#1->  0,0,5                         ..e\n",
    "#2->  0,5,13                        .em\n",
    "#3->  5,13,13                       emm\n",
    "#4->  13,13,1                       mma\n",
    "#(No entry for ma.)\n",
    "\n",
    "\n",
    "#2. a. outputCharcterForThatContext: Represents an array of size equal to number of rows in inputContext and outputCharcterForThatContext[i]\n",
    "#gives index the next character for the sliding window or the context represented by ith row in inputContext\n",
    "#b. We can say outputCharcterForThatContext is label for our data so we have labelled dataset for our training\n",
    "inputContext = torch.tensor(inputContext)\n",
    "outputCharcterForThatContext = torch.tensor(outputCharcterForThatContext)\n",
    "print(inputContext.shape)\n",
    "print(outputCharcterForThatContext.shape)\n",
    "\n",
    "\n",
    "#3. We can change size of sliding window to 4 or 5 or anything in that case our contextWindow will be ....., ....e, ...em, ..emm, .emma,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2b9b21df-e21b-4e1b-a76a-5c002615457b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.7745, 0.2319], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.7745, 0.2319], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3 a.Creating Embedding for Each Character so that it can be comfortably for training ML Model like matrix multiplication\n",
    "#b. In bigram model we created one hot encoding i.e each character was written as 27-bit \n",
    "#c. Here we will map each character to 2 dimensional embedding, so for 27 characters we will get 27x2 matrix\n",
    "\n",
    "charEmbeddings = torch.randn((27,2),requires_grad=True)\n",
    "\n",
    "charEmbeddings[5] # this gives 2 dimensional embedding of char at index:5  i.e e\n",
    "print(charEmbeddings[5])\n",
    "\n",
    "#e. charEmbeddings[5] THIS EMBEDDING IS EQUIVALENT TO:\n",
    "embedding = F.one_hot(torch.tensor(5), num_classes=27).float() @ charEmbeddings \n",
    "embedding\n",
    "#So we can see charEmbeddings[5] = embedding \n",
    "# 1. We created one hot encoding of e as 000010...0 (i.e 1x27 matrix) and did matrix multiplication with 27x2 matrix(this matrix is like \n",
    "  #weights matrix so we can say our charEmbedding is similar 1 layer neural , so this is first layer of neural net\n",
    "# 2. so we will be using charEmbedding directly instead of this one hot encoding and matrix multiplication\n",
    "\n",
    "\n",
    "# charEmbeddings will be used as look up table to find embedding of a char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e3407a90-e028-47a1-80e1-cd75c4e54f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 3, 2])\n",
      "torch.Size([228146, 6])\n"
     ]
    }
   ],
   "source": [
    "#4. Preparing Input data for neural network which will go to layer-1\n",
    "inputForLayer1 = charEmbeddings[inputContext]\n",
    "print(inputForLayer1.shape) # 3-d dimensional(p,q,r) where p-> number of context window possible for all names, q-> size of context window\n",
    "#basically block_size, r-> the dimension in which each char is embedded so in our case we have embedded each char in 2D(Look at block-2)\n",
    "\n",
    "#Flattening from 3d to 2d\n",
    "inputForLayer1 = inputForLayer1.view(len(inputContext), 6)\n",
    "print(inputForLayer1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "edae43af-ba63-4f0c-b517-2c11c7f8c5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. DEFINING TWO LAYERS OF NEURAL NETWORK\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "weightLayer1 = torch.randn((6, 100), generator=g, requires_grad=True) #layer-1 100 neurons each neuron have 6 weights because input have 6 feuture so we have\n",
    "#6 weights\n",
    "biasLayer1 = torch.randn(100, generator=g, requires_grad=True) \n",
    "\n",
    "weightLayer2 = torch.randn((100, 27), generator=g, requires_grad=True) # layer-2 27 neurons each neurons have 100 weights because input for layer-2 will havve\n",
    "#100 feature so we have 100 weight (228146,6 X 6,100--> 228146,100 Matrix which is input for layer-2)\n",
    "biasLayer2 = torch.randn(27, generator=g, requires_grad=True) #layer-2 27 biases\n",
    "parameters = [charEmbeddings, weightLayer1, biasLayer1, weightLayer2, biasLayer2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cc913774-b866-4c9a-af8e-905166aa0343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manually calculating loss tensor(18.8199, grad_fn=<NegBackward0>)\n",
      "calculating loss through cross entropy tensor(18.8199, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#6. RUNING UNTRAINED NEURAL NET TO FOR FIRST TIME TO GET RESULT(FORWARD PASS)\n",
    "\n",
    "h = torch.tanh(inputForLayer1@weightLayer1+biasLayer1) #Output from Layer-1, Range of tanh=[-1,1]\n",
    "#layer-1 100 biases, bias should be equal to number of neurons in layer\n",
    "# as o/p from neuron = Summation(wi*xi)+b (if you see each cell of output matrix after multiplication is summation(wi*xi)) and then we add\n",
    "#bias to each cell of o/p matrix\n",
    "\n",
    "logits = h@weightLayer2+biasLayer2 #Output from Layer-2\n",
    "\n",
    "counts = logits.exp()\n",
    "prob = counts/counts.sum(1,keepdims=True) #prob[i][j] = count[i][j]/(sum of elements in count[i] row)\n",
    "loss = -prob[torch.arange(len(inputContext)),outputCharcterForThatContext].log().mean()\n",
    "loss\n",
    "print(\"manually calculating loss\",loss)\n",
    "\n",
    "#b. ShortCut for calculation loss using CROSS ENTROPY:\n",
    "loss = F.cross_entropy(logits,outputCharcterForThatContext)\n",
    "print(\"calculating loss through cross entropy\",loss)\n",
    "\n",
    "#We should always use cross entropy function because of following reasons:\n",
    "    #1. Manual calculation result in creating extra tensors like counts and prob which can be heavy for large dataset\n",
    "    #2. Pytorch optimises loss calculation using Cross entropy because it clusterup various operation\n",
    "    #3. VERY IMP REASON:\n",
    "        #3.1 we do exponentiation of each cell of logits matrix to make counts positive value but if suppose logits[i][j] = 100 or \n",
    "        #any slightly big value then e^100 will be cross limits of float datatype and hence count[i][j] = infinity and our prob matrix will go for \n",
    "        #toss\n",
    "        #3.2 How Cross Entropy solves this problems:\n",
    "        #It finds max value in logits[][] and subtract each element with that value, now since count[][] is normalised hence there will be no impact\n",
    "        #on count[i][j](bec we are subtracting each element with same value so there will be no impact on normalised value) because of this \n",
    "        #optimisation no element can have more than 0 value so no chance of e^x crossing floats limit\n",
    "    #4. Backprob is easier in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f953e358-deed-4cb0-88c2-6c0419a76b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method item of Tensor object at 0x164fdde50>\n",
      "<built-in method item of Tensor object at 0x164fdd090>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters:\n\u001b[1;32m      9\u001b[0m     p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m#We will need to optimise charEmbedding(because it has 2 dimension for 27 chars, weights and biases\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters:\n\u001b[1;32m     12\u001b[0m     p\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.1\u001b[39m\u001b[38;5;241m*\u001b[39mp\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "#7. TRAINING THE MODEL( FORWARD PASS AND BACKWARD PASS FOR SOME NUMBER OF TIME)\n",
    "for _ in range(10):\n",
    "    #forward pass\n",
    "    h = torch.tanh(inputForLayer1@weightLayer1+biasLayer1)\n",
    "    logits = h@weightLayer2+biasLayer2\n",
    "    loss = F.cross_entropy(logits,outputCharcterForThatContext)\n",
    "    print(loss.item)\n",
    "    for p in parameters:\n",
    "        p.grad = None #We will need to optimise charEmbedding(because it has 2 dimension for 27 chars, weights and biases\n",
    "    loss.backward()\n",
    "    for p in parameters:\n",
    "        p.data += -0.1*p.grad\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ed3799-6394-4ad5-bc44-e393fb698b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
