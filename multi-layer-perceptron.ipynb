{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf957494-499b-42c5-9b40-b758d85baa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8d59ff-f57c-4479-be0d-d3ff1b2af68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBJECTIVE: WE WANT TO PREDICT NEXT CHARATCER BY TAKING CONTEXT FROM MORE PREVIOUS CHARACTERS(like 3) INSTEAD OF JUST 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f4acce9-9688-44a4-9004-bd5f9d24b8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1 read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82fe25ad-337b-4cd9-8735-0d29a03f3d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "charIntIndexMapping = {s:i+1 for i,s in enumerate(chars)}\n",
    "charIntIndexMapping['.'] = 0\n",
    "indexToCharMapping ={i:s for s,i in charIntIndexMapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d8758cc-88fb-4fbf-96d0-2b111083321c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 3])\n",
      "torch.Size([228146])\n"
     ]
    }
   ],
   "source": [
    "#2 BUILDING TRAINING DATASET\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one? basically size of sliding window\n",
    "inputContext, outputCharcterForThatContext = [], []\n",
    "\n",
    "for w in words:\n",
    "  \n",
    "\n",
    "  contextSlidingWindow = [0] * block_size #Initialise with arr= [0,0,0] where 0 represents '.' char so essentially \"...\"\n",
    "    \n",
    "  for ch in w + '.': #Appending '.' to mark end of word\n",
    "      \n",
    "    #NOTE ON APPENDING '.' in Words:\n",
    "    #1. In bigram model we were appending '.' to start and end of each word \n",
    "    #2. Here we dont need to append '.' in starting because we are starting context as \"...\" \n",
    "\n",
    "      \n",
    "    index = charIntIndexMapping[ch]\n",
    "    inputContext.append(contextSlidingWindow)\n",
    "    outputCharcterForThatContext.append(index)\n",
    "    #print(''.join(indexToCharMapping[i] for i in context), '--->', indexToCharMapping[ix])\n",
    "    \n",
    "    contextSlidingWindow = contextSlidingWindow[1:] + [index] #Updating context, so context is like sliding window of size=block_size, when you move to \n",
    "    #next iteration you append current char from back in sliding window and first char is removed\n",
    "    #Syntax context[1:] means taking array from index-1 to last index and appending index represented by current char\n",
    "\n",
    "#1. inputContext: Represents a matrix where row represents integer form of each possible sliding window of size -3 in all name in names.txt\n",
    "# and we have 3 columns(bec context_length=3) which represents the content of contextWindow represented by row\n",
    "\n",
    "\n",
    "#Forexample for emma actually becomes emma.\n",
    "#Row  Columns(int mapping of char)  contextWindow  \n",
    "#0->  0,0,0                         ...\n",
    "#1->  0,0,5                         ..e\n",
    "#2->  0,5,13                        .em\n",
    "#3->  5,13,13                       emm\n",
    "#4->  13,13,1                       mma\n",
    "#(No entry for ma.)\n",
    "\n",
    "\n",
    "#2. a. outputCharcterForThatContext: Represents an array of size equal to number of rows in inputContext and outputCharcterForThatContext[i]\n",
    "#gives index the next character for the sliding window or the context represented by ith row in inputContext\n",
    "#b. We can say outputCharcterForThatContext is label for our data so we have labelled dataset for our training\n",
    "inputContext = torch.tensor(inputContext)\n",
    "outputCharcterForThatContext = torch.tensor(outputCharcterForThatContext)\n",
    "print(inputContext.shape)\n",
    "print(outputCharcterForThatContext.shape)\n",
    "\n",
    "\n",
    "#3. We can change size of sliding window to 4 or 5 or anything in that case our contextWindow will be ....., ....e, ...em, ..emm, .emma,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9b21df-e21b-4e1b-a76a-5c002615457b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
