{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf957494-499b-42c5-9b40-b758d85baa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa8d59ff-f57c-4479-be0d-d3ff1b2af68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBJECTIVE: WE WANT TO PREDICT NEXT CHARATCER BY TAKING CONTEXT FROM MORE PREVIOUS CHARACTERS(like 3) INSTEAD OF JUST 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f4acce9-9688-44a4-9004-bd5f9d24b8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1 read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82fe25ad-337b-4cd9-8735-0d29a03f3d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "charIntIndexMapping = {s:i+1 for i,s in enumerate(chars)}\n",
    "charIntIndexMapping['.'] = 0\n",
    "indexToCharMapping ={i:s for s,i in charIntIndexMapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d8758cc-88fb-4fbf-96d0-2b111083321c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 3])\n",
      "torch.Size([228146])\n"
     ]
    }
   ],
   "source": [
    "#2 BUILDING TRAINING DATASET\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one? basically size of sliding window\n",
    "inputContext, outputCharcterForThatContext = [], []\n",
    "\n",
    "for w in words:\n",
    "  \n",
    "\n",
    "  contextSlidingWindow = [0] * block_size #Initialise with arr= [0,0,0] where 0 represents '.' char so essentially \"...\"\n",
    "    \n",
    "  for ch in w + '.': #Appending '.' to mark end of word\n",
    "      \n",
    "    #NOTE ON APPENDING '.' in Words:\n",
    "    #1. In bigram model we were appending '.' to start and end of each word \n",
    "    #2. Here we dont need to append '.' in starting because we are starting context as \"...\" \n",
    "\n",
    "      \n",
    "    index = charIntIndexMapping[ch]\n",
    "    inputContext.append(contextSlidingWindow)\n",
    "    outputCharcterForThatContext.append(index)\n",
    "    #print(''.join(indexToCharMapping[i] for i in context), '--->', indexToCharMapping[ix])\n",
    "    \n",
    "    contextSlidingWindow = contextSlidingWindow[1:] + [index] #Updating context, so context is like sliding window of size=block_size, when you move to \n",
    "    #next iteration you append current char from back in sliding window and first char is removed\n",
    "    #Syntax context[1:] means taking array from index-1 to last index and appending index represented by current char\n",
    "\n",
    "#1. inputContext: Represents a matrix where row represents integer form of each possible sliding window of size -3 in all name in names.txt\n",
    "# and we have 3 columns(bec context_length=3) which represents the content of contextWindow represented by row\n",
    "\n",
    "\n",
    "#Forexample for emma actually becomes emma.\n",
    "#Row  Columns(int mapping of char)  contextWindow  \n",
    "#0->  0,0,0                         ...\n",
    "#1->  0,0,5                         ..e\n",
    "#2->  0,5,13                        .em\n",
    "#3->  5,13,13                       emm\n",
    "#4->  13,13,1                       mma\n",
    "#(No entry for ma.)\n",
    "\n",
    "\n",
    "#2. a. outputCharcterForThatContext: Represents an array of size equal to number of rows in inputContext and outputCharcterForThatContext[i]\n",
    "#gives index the next character for the sliding window or the context represented by ith row in inputContext\n",
    "#b. We can say outputCharcterForThatContext is label for our data so we have labelled dataset for our training\n",
    "inputContext = torch.tensor(inputContext)\n",
    "outputCharcterForThatContext = torch.tensor(outputCharcterForThatContext)\n",
    "print(inputContext.shape)\n",
    "print(outputCharcterForThatContext.shape)\n",
    "\n",
    "\n",
    "#3. We can change size of sliding window to 4 or 5 or anything in that case our contextWindow will be ....., ....e, ...em, ..emm, .emma,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b9b21df-e21b-4e1b-a76a-5c002615457b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3084, -0.7322], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.3084, -0.7322], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3 a.Creating Embedding for Each Character so that it can be comfortably for training ML Model like matrix multiplication\n",
    "#b. In bigram model we created one hot encoding i.e each character was written as 27-bit \n",
    "#c. Here we will map each character to 2 dimensional embedding, so for 27 characters we will get 27x2 matrix\n",
    "\n",
    "charEmbeddings = torch.randn((27,2),requires_grad=True)\n",
    "\n",
    "charEmbeddings[5] # this gives 2 dimensional embedding of char at index:5  i.e e\n",
    "print(charEmbeddings[5])\n",
    "\n",
    "#e. charEmbeddings[5] THIS EMBEDDING IS EQUIVALENT TO:\n",
    "embedding = F.one_hot(torch.tensor(5), num_classes=27).float() @ charEmbeddings \n",
    "embedding\n",
    "#So we can see charEmbeddings[5] = embedding \n",
    "# 1. We created one hot encoding of e as 000010...0 (i.e 1x27 matrix) and did matrix multiplication with 27x2 matrix(this matrix is like \n",
    "  #weights matrix so we can say our charEmbedding is similar 1 layer neural , so this is first layer of neural net\n",
    "# 2. so we will be using charEmbedding directly instead of this one hot encoding and matrix multiplication\n",
    "\n",
    "\n",
    "# charEmbeddings will be used as look up table to find embedding of a char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3407a90-e028-47a1-80e1-cd75c4e54f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 3, 2])\n",
      "torch.Size([228146, 6])\n"
     ]
    }
   ],
   "source": [
    "#4. Preparing Input data for neural network which will go to layer-1\n",
    "inputForLayer1 = charEmbeddings[inputContext]\n",
    "print(inputForLayer1.shape) # 3-d dimensional(p,q,r) where p-> number of context window possible for all names, q-> size of context window\n",
    "#basically block_size, r-> the dimension in which each char is embedded so in our case we have embedded each char in 2D(Look at block-2)\n",
    "\n",
    "#Flattening from 3d to 2d\n",
    "inputForLayer1 = inputForLayer1.view(len(inputContext), 6)\n",
    "print(inputForLayer1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "edae43af-ba63-4f0c-b517-2c11c7f8c5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. DEFINING TWO LAYERS OF NEURAL NETWORK\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "weightLayer1 = torch.randn((6, 100), generator=g, requires_grad=True) #layer-1 100 neurons each neuron have 6 weights because input have 6 feuture so we have\n",
    "#6 weights\n",
    "biasLayer1 = torch.randn(100, generator=g, requires_grad=True) \n",
    "\n",
    "weightLayer2 = torch.randn((100, 27), generator=g, requires_grad=True) # layer-2 27 neurons each neurons have 100 weights because input for layer-2 will havve\n",
    "#100 feature so we have 100 weight (228146,6 X 6,100--> 228146,100 Matrix which is input for layer-2)\n",
    "biasLayer2 = torch.randn(27, generator=g, requires_grad=True) #layer-2 27 biases\n",
    "parameters = [charEmbeddings, weightLayer1, biasLayer1, weightLayer2, biasLayer2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc913774-b866-4c9a-af8e-905166aa0343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manually calculating loss tensor(17.2707, grad_fn=<NegBackward0>)\n",
      "calculating loss through cross entropy tensor(17.2707, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#6. RUNING UNTRAINED NEURAL NET TO FOR FIRST TIME TO GET RESULT(FORWARD PASS)\n",
    "\n",
    "h = torch.tanh(inputForLayer1@weightLayer1+biasLayer1) #Output from Layer-1, Range of tanh=[-1,1]\n",
    "#layer-1 100 biases, bias should be equal to number of neurons in layer\n",
    "# as o/p from neuron = Summation(wi*xi)+b (if you see each cell of output matrix after multiplication is summation(wi*xi)) and then we add\n",
    "#bias to each cell of o/p matrix\n",
    "\n",
    "logits = h@weightLayer2+biasLayer2 #Output from Layer-2\n",
    "\n",
    "counts = logits.exp()\n",
    "prob = counts/counts.sum(1,keepdims=True) #prob[i][j] = count[i][j]/(sum of elements in count[i] row)\n",
    "loss = -prob[torch.arange(len(inputContext)),outputCharcterForThatContext].log().mean()\n",
    "loss\n",
    "print(\"manually calculating loss\",loss)\n",
    "\n",
    "#b. ShortCut for calculation loss using CROSS ENTROPY:\n",
    "loss = F.cross_entropy(logits,outputCharcterForThatContext)\n",
    "print(\"calculating loss through cross entropy\",loss)\n",
    "\n",
    "#We should always use cross entropy function because of following reasons:\n",
    "    #1. Manual calculation result in creating extra tensors like counts and prob which can be heavy for large dataset\n",
    "    #2. Pytorch optimises loss calculation using Cross entropy because it clusterup various operation\n",
    "    #3. VERY IMP REASON:\n",
    "        #3.1 we do exponentiation of each cell of logits matrix to make counts positive value but if suppose logits[i][j] = 100 or \n",
    "        #any slightly big value then e^100 will be cross limits of float datatype and hence count[i][j] = infinity and our prob matrix will go for \n",
    "        #toss\n",
    "        #3.2 How Cross Entropy solves this problems:\n",
    "        #It finds max value in logits[][] and subtract each element with that value, now since count[][] is normalised hence there will be no impact\n",
    "        #on count[i][j](bec we are subtracting each element with same value so there will be no impact on normalised value) because of this \n",
    "        #optimisation no element can have more than 0 value so no chance of e^x crossing floats limit\n",
    "    #4. Backprob is easier in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea9602d4-260f-457f-b445-5ce2edd527e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "charEmbeddings = torch.randn((27,2),generator=g,requires_grad=True)\n",
    "weightLayer1 = torch.randn((6, 100), generator=g, requires_grad=True)\n",
    "biasLayer1 = torch.randn(100, generator=g, requires_grad=True)\n",
    "weightLayer2 = torch.randn((100, 27), generator=g, requires_grad=True)\n",
    "biasLayer2 = torch.randn(27, generator=g, requires_grad=True)\n",
    "parameters = [weightLayer1, biasLayer1, weightLayer2, biasLayer2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f953e358-deed-4cb0-88c2-6c0419a76b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "19.48078155517578\n",
      "2\n",
      "17.05489730834961\n",
      "3\n",
      "15.107970237731934\n",
      "4\n",
      "13.877887725830078\n",
      "5\n",
      "13.149697303771973\n",
      "6\n",
      "12.570921897888184\n",
      "7\n",
      "12.109742164611816\n",
      "8\n",
      "11.702702522277832\n",
      "9\n",
      "11.3184814453125\n",
      "10\n",
      "10.954071998596191\n"
     ]
    }
   ],
   "source": [
    "#7. TRAINING THE MODEL( FORWARD PASS AND BACKWARD PASS FOR SOME NUMBER OF TIME)\n",
    "count=0\n",
    "for _ in range(10):\n",
    "    count = count+1\n",
    "    print(count)\n",
    "    inputForLayerOne = charEmbeddings[inputContext]\n",
    "    inputForLayerOne = inputForLayer1.view(len(inputContext), 6)\n",
    "    #forward pass\n",
    "    h = torch.tanh(inputForLayerOne@weightLayer1+biasLayer1)\n",
    "    logits = h@weightLayer2+biasLayer2\n",
    "    loss = F.cross_entropy(logits,outputCharcterForThatContext)\n",
    "    print(loss.item())\n",
    "    for p in parameters:\n",
    "        p.grad = None #We will need to optimise charEmbedding(because it has 2 dimension for 27 chars, weights and biases\n",
    "    loss.backward(retain_graph=True)\n",
    "    for p in parameters:\n",
    "        p.data += -0.1*p.grad\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ed3799-6394-4ad5-bc44-e393fb698b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
