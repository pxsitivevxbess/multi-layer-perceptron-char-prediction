{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf957494-499b-42c5-9b40-b758d85baa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8d59ff-f57c-4479-be0d-d3ff1b2af68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBJECTIVE: WE WANT TO PREDICT NEXT CHARATCER BY TAKING CONTEXT FROM MORE PREVIOUS CHARACTERS(like 3) INSTEAD OF JUST 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f4acce9-9688-44a4-9004-bd5f9d24b8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1 read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82fe25ad-337b-4cd9-8735-0d29a03f3d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "charIntIndexMapping = {s:i+1 for i,s in enumerate(chars)}\n",
    "charIntIndexMapping['.'] = 0\n",
    "indexToCharMapping ={i:s for s,i in charIntIndexMapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d8758cc-88fb-4fbf-96d0-2b111083321c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 3])\n",
      "torch.Size([228146])\n"
     ]
    }
   ],
   "source": [
    "#2 BUILDING TRAINING DATASET\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one? basically size of sliding window\n",
    "inputContext, outputCharcterForThatContext = [], []\n",
    "\n",
    "for w in words:\n",
    "  \n",
    "\n",
    "  contextSlidingWindow = [0] * block_size #Initialise with arr= [0,0,0] where 0 represents '.' char so essentially \"...\"\n",
    "    \n",
    "  for ch in w + '.': #Appending '.' to mark end of word\n",
    "      \n",
    "    #NOTE ON APPENDING '.' in Words:\n",
    "    #1. In bigram model we were appending '.' to start and end of each word \n",
    "    #2. Here we dont need to append '.' in starting because we are starting context as \"...\" \n",
    "\n",
    "      \n",
    "    index = charIntIndexMapping[ch]\n",
    "    inputContext.append(contextSlidingWindow)\n",
    "    outputCharcterForThatContext.append(index)\n",
    "    #print(''.join(indexToCharMapping[i] for i in context), '--->', indexToCharMapping[ix])\n",
    "    \n",
    "    contextSlidingWindow = contextSlidingWindow[1:] + [index] #Updating context, so context is like sliding window of size=block_size, when you move to \n",
    "    #next iteration you append current char from back in sliding window and first char is removed\n",
    "    #Syntax context[1:] means taking array from index-1 to last index and appending index represented by current char\n",
    "\n",
    "#1. inputContext: Represents a matrix where row represents integer form of each possible sliding window of size -3 in all name in names.txt\n",
    "# and we have 3 columns(bec context_length=3) which represents the content of contextWindow represented by row\n",
    "\n",
    "\n",
    "#Forexample for emma actually becomes emma.\n",
    "#Row  Columns(int mapping of char)  contextWindow  \n",
    "#0->  0,0,0                         ...\n",
    "#1->  0,0,5                         ..e\n",
    "#2->  0,5,13                        .em\n",
    "#3->  5,13,13                       emm\n",
    "#4->  13,13,1                       mma\n",
    "#(No entry for ma.)\n",
    "\n",
    "\n",
    "#2. a. outputCharcterForThatContext: Represents an array of size equal to number of rows in inputContext and outputCharcterForThatContext[i]\n",
    "#gives index the next character for the sliding window or the context represented by ith row in inputContext\n",
    "#b. We can say outputCharcterForThatContext is label for our data so we have labelled dataset for our training\n",
    "inputContext = torch.tensor(inputContext)\n",
    "outputCharcterForThatContext = torch.tensor(outputCharcterForThatContext)\n",
    "print(inputContext.shape)\n",
    "print(outputCharcterForThatContext.shape)\n",
    "\n",
    "\n",
    "#3. We can change size of sliding window to 4 or 5 or anything in that case our contextWindow will be ....., ....e, ...em, ..emm, .emma,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b9b21df-e21b-4e1b-a76a-5c002615457b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0274, -0.0186])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.0274, -0.0186])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3 a.Creating Embedding for Each Character so that it can be comfortably for training ML Model like matrix multiplication\n",
    "#b. In bigram model we created one hot encoding i.e each character was written as 27-bit \n",
    "#c. Here we will map each character to 2 dimensional embedding, so for 27 characters we will get 27x2 matrix\n",
    "\n",
    "charEmbeddings = torch.randn((27,2))\n",
    "\n",
    "charEmbeddings[5] # this gives 2 dimensional embedding of char at index:5  i.e e\n",
    "print(charEmbeddings[5])\n",
    "\n",
    "#e. charEmbeddings[5] THIS EMBEDDING IS EQUIVALENT TO:\n",
    "embedding = F.one_hot(torch.tensor(5), num_classes=27).float() @ charEmbeddings \n",
    "embedding\n",
    "#So we can see charEmbeddings[5] = embedding \n",
    "# 1. We created one hot encoding of e as 000010...0 (i.e 1x27 matrix) and did matrix multiplication with 27x2 matrix(this matrix is like \n",
    "  #weights matrix so we can say our charEmbedding is similar 1 layer neural , so this is first layer of neural net\n",
    "# 2. so we will be using charEmbedding directly instead of this one hot encoding and matrix multiplication\n",
    "\n",
    "\n",
    "# charEmbeddings will be used as look up table to find embedding of a char\n",
    "\n",
    "charEmbeddings[inputContext]["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e3407a90-e028-47a1-80e1-cd75c4e54f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 3, 2])\n",
      "torch.Size([228146, 6])\n"
     ]
    }
   ],
   "source": [
    "#4. Preparing Input data for neural network which will go to layer-1\n",
    "inputForLayer1 = charEmbeddings[inputContext]\n",
    "print(inputForLayer1.shape) # 3-d dimensional(p,q,r) where p-> number of context window possible for all names, q-> size of context window\n",
    "#basically block_size, r-> the dimension in which each char is embedded so in our case we have embedded each char in 2D(Look at block-2)\n",
    "\n",
    "#Flattening from 3d to 2d\n",
    "inputForLayer1 = inputForLayer1.view(len(inputContext), 6)\n",
    "print(inputForLayer1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "edae43af-ba63-4f0c-b517-2c11c7f8c5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. DEFINING TWO LAYERS OF NEURAL NETWORK\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "weightLayer1 = torch.randn((6, 100), generator=g) #layer-1 100 neurons each neuron have 6 weights because input have 6 feuture so we have\n",
    "#6 weights\n",
    "biasLayer1 = torch.randn(100, generator=g) #layer-1 100 biases\n",
    "weightLayer2 = torch.randn((100, 27), generator=g) # layer-2 27 neurons each neurons have 100 weights because input for layer-2 will havve\n",
    "#100 feature so we have 100 weight ())\n",
    "biasLayer2 = torch.randn(27, generator=g) #layer-2 27 biases\n",
    "parameters = [charEmbeddings, weightLayer1, biasLayer1, weightLayer2, biasLayer2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc913774-b866-4c9a-af8e-905166aa0343",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
